1) ShallowConv model successfully converged on the first 500 sentences of the 
top_thou dataset. 

exp:
optim - Adam lr = 0.0001
lr_scheduler - None
epochs - ~300 to converge
file - conv_model_exp.py

PHASE 1 EXPERIMENTS:
Dataset - short_clips => 41000 clips

1) Shallow conv:
	file - shallow_conv_exp.py
	epochs - 19
	loss - 2.981
	result - The network did not converge as fast as other networks, 
		loss was at 2.981 and has stagnated. The network may have
		been in the phase of refining output activations before having
		observation specific outputs be generated, but at the time of
		stopping it was only outputting "ghe" for all samples.


2) Standard conv:
	file - standard_conv_exp.py
	epochs - 20
	loss - 0.95
	result - The network was able to reach a loss of 0.95 and showed 
		 strong evidence of learning. The network will most likely
		 converge to a good solution if trained fully.


3) Other model:
	file - other_model_exp.py
	epochs - 7
	loss - 0.56
	result - Best model out of group.

4) Simple RNN:
	file - simple_rnn_exp.py
	epochs - 2
	loss 2.9
	result - Model failed to learn in earlier experiments so it is most
		 likely going to fail again.


PHASE 2 EXPERIMENTS:

1) LayerNorm only:
	file - norm_conv_exp.py
	epochs - 7
	loss - 2.542 

2) LSTM only:
	file - lstm_conv_exp.py
	epochs - 7
	loss - 1.516

3) LSTM + LayerNorm:
	file - lstm_norm_conv_exp.py
	epochs - 7
	loss - 1.379 


4) Residual blocks:
	file - res_conv_exp.py
	epochs - 7
	loss - 1.443 

5) Res + LSTM + LayerNorm:
	file - res_lstm_conv_exp.py
	epochs - 7
	loss - 1.139

6) Other model:
	file - 
	epochs - 7
	loss - 
